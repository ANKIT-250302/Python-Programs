{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMRnJ9v6uQGO9/4CQiD9Mlc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#TOKENIZER"],"metadata":{"id":"cCFtRtMgk7Q2"}},{"cell_type":"markdown","source":["Tokenization is the process of dividing a text into smaller units known as tokens. Tokens are typically words or sub-words in the context of natural language processing. Tokenization is a critical step in many NLP tasks, including text processing, language modelling, and machine translation. The process involves splitting a string, or text into a list of tokens. One can think of tokens as parts like a word is a token in a sentence, and a sentence is a token in a paragraph."],"metadata":{"id":"KMDo0K4gAuju"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"IqiGeJ0fveia","executionInfo":{"status":"ok","timestamp":1720589592594,"user_tz":-330,"elapsed":455,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}}},"outputs":[],"source":["corpus=\"\"\"\n","  Tokenization is the process of tokenizing or splitting a string, text into a list of tokens. One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph.\n","\"\"\""]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xK689iJOwahv","executionInfo":{"status":"ok","timestamp":1720589597314,"user_tz":-330,"elapsed":4049,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}},"outputId":"5c82e045-37ee-44dc-c4e2-6d1b94ee8d0b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["nltk.download('')"],"metadata":{"id":"YYW9S-BByvVp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720589597314,"user_tz":-330,"elapsed":18,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}},"outputId":"f0624d28-42d4-4963-eaff-f06c73fe389e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Error loading : Package '' not found in index\n"]},{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["##Types of Tokenization\n","1.Word tokenizaton:- Word tokenization divides the text into individual words. Many NLP tasks use this approach, in which words are treated as the basic units of meaning."],"metadata":{"id":"2mKyPLA-BOGf"}},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize\n","words=word_tokenize(corpus)\n"],"metadata":{"id":"RPKcG0FnhcMS","executionInfo":{"status":"ok","timestamp":1720589597314,"user_tz":-330,"elapsed":15,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["print(words)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RZL_7lWmkBL3","executionInfo":{"status":"ok","timestamp":1720589597314,"user_tz":-330,"elapsed":14,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}},"outputId":"8bf336dd-7ed6-4737-88f5-23698ccf91f0"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["['Tokenization', 'is', 'the', 'process', 'of', 'tokenizing', 'or', 'splitting', 'a', 'string', ',', 'text', 'into', 'a', 'list', 'of', 'tokens', '.', 'One', 'can', 'think', 'of', 'token', 'as', 'parts', 'like', 'a', 'word', 'is', 'a', 'token', 'in', 'a', 'sentence', ',', 'and', 'a', 'sentence', 'is', 'a', 'token', 'in', 'a', 'paragraph', '.']\n"]}]},{"cell_type":"markdown","source":["2.Sentence Tokenization:- The text is segmented into sentences during sentence tokenization. This is useful for tasks requiring individual sentence analysis or processing.\n","\n","Example:"],"metadata":{"id":"lirEJbozBbQE"}},{"cell_type":"code","source":["from nltk.tokenize import sent_tokenize\n","sent_tokenize(corpus)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j1NplRb5h7f0","executionInfo":{"status":"ok","timestamp":1720589597314,"user_tz":-330,"elapsed":12,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}},"outputId":"c7195c5b-4003-4f40-e769-15f4e4b0e1ea"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['\\n  Tokenization is the process of tokenizing or splitting a string, text into a list of tokens.',\n"," 'One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph.']"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["##Stemming\n","Stemming generates the base word from the inflected word by removing the affixes of the word. It has a set of pre-defined rules that govern the dropping of these affixes. It must be noted that stemmers might not always result in semantically meaningful base words."],"metadata":{"id":"hjU-1JhxCAdd"}},{"cell_type":"markdown","source":["#STEMMING"],"metadata":{"id":"UXFI-7TjlDKH"}},{"cell_type":"code","source":["from nltk.stem import PorterStemmer\n","ps=PorterStemmer()"],"metadata":{"id":"et6YVZnTlGcw","executionInfo":{"status":"ok","timestamp":1720589597315,"user_tz":-330,"elapsed":11,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["for i in words:\n","  print(i+\"--->\"+ps.stem(i ))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DT67p9-7lly-","executionInfo":{"status":"ok","timestamp":1720589597315,"user_tz":-330,"elapsed":10,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}},"outputId":"ba4a4453-defd-4d99-af37-c024e2a1eaa8"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenization--->token\n","is--->is\n","the--->the\n","process--->process\n","of--->of\n","tokenizing--->token\n","or--->or\n","splitting--->split\n","a--->a\n","string--->string\n",",--->,\n","text--->text\n","into--->into\n","a--->a\n","list--->list\n","of--->of\n","tokens--->token\n",".--->.\n","One--->one\n","can--->can\n","think--->think\n","of--->of\n","token--->token\n","as--->as\n","parts--->part\n","like--->like\n","a--->a\n","word--->word\n","is--->is\n","a--->a\n","token--->token\n","in--->in\n","a--->a\n","sentence--->sentenc\n",",--->,\n","and--->and\n","a--->a\n","sentence--->sentenc\n","is--->is\n","a--->a\n","token--->token\n","in--->in\n","a--->a\n","paragraph--->paragraph\n",".--->.\n"]}]},{"cell_type":"markdown","source":["#LAncaster Stemmming"],"metadata":{"id":"P3PDXSmgmRUX"}},{"cell_type":"code","source":["from nltk.stem import LancasterStemmer\n","ls=LancasterStemmer()"],"metadata":{"id":"9UWzdk-_mLPW","executionInfo":{"status":"ok","timestamp":1720589597315,"user_tz":-330,"elapsed":8,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["for i in words:\n","  print(i+\"--->\"+ls.stem(i ))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aIAev0JBmhQq","executionInfo":{"status":"ok","timestamp":1720589597315,"user_tz":-330,"elapsed":8,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}},"outputId":"97e39030-811a-4d75-ea43-b974913b24b3"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenization--->tok\n","is--->is\n","the--->the\n","process--->process\n","of--->of\n","tokenizing--->tok\n","or--->or\n","splitting--->splitting\n","a--->a\n","string--->string\n",",--->,\n","text--->text\n","into--->into\n","a--->a\n","list--->list\n","of--->of\n","tokens--->tok\n",".--->.\n","One--->on\n","can--->can\n","think--->think\n","of--->of\n","token--->tok\n","as--->as\n","parts--->part\n","like--->lik\n","a--->a\n","word--->word\n","is--->is\n","a--->a\n","token--->tok\n","in--->in\n","a--->a\n","sentence--->sent\n",",--->,\n","and--->and\n","a--->a\n","sentence--->sent\n","is--->is\n","a--->a\n","token--->tok\n","in--->in\n","a--->a\n","paragraph--->paragraph\n",".--->.\n"]}]},{"cell_type":"code","source":[" from nltk.stem import RegexpStemmer\n"," rs=RegexpStemmer(\"ing&|able$\")"],"metadata":{"id":"LKrIMxJlmodq","executionInfo":{"status":"ok","timestamp":1720589597315,"user_tz":-330,"elapsed":6,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["for i in words:\n","  print(i+\"--->\"+rs.stem(i ))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"duAnJ0uAnNO0","executionInfo":{"status":"ok","timestamp":1720589597315,"user_tz":-330,"elapsed":6,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}},"outputId":"67ef11ee-a31f-4f5d-9603-a8ec4937b2cf"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenization--->Tokenization\n","is--->is\n","the--->the\n","process--->process\n","of--->of\n","tokenizing--->tokenizing\n","or--->or\n","splitting--->splitting\n","a--->a\n","string--->string\n",",--->,\n","text--->text\n","into--->into\n","a--->a\n","list--->list\n","of--->of\n","tokens--->tokens\n",".--->.\n","One--->One\n","can--->can\n","think--->think\n","of--->of\n","token--->token\n","as--->as\n","parts--->parts\n","like--->like\n","a--->a\n","word--->word\n","is--->is\n","a--->a\n","token--->token\n","in--->in\n","a--->a\n","sentence--->sentence\n",",--->,\n","and--->and\n","a--->a\n","sentence--->sentence\n","is--->is\n","a--->a\n","token--->token\n","in--->in\n","a--->a\n","paragraph--->paragraph\n",".--->.\n"]}]},{"cell_type":"code","source":["from nltk.stem import SnowballStemmer\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","ss=SnowballStemmer(\"english\",ignore_stopwords=True)\n","for i in words:\n","  print(i+\"--->\"+ss.stem(i ))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WQifdYnynram","executionInfo":{"status":"ok","timestamp":1720589657459,"user_tz":-330,"elapsed":454,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}},"outputId":"cb6a397c-5a0b-4c06-bd2f-de8fe86439fe"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenization--->token\n","is--->is\n","the--->the\n","process--->process\n","of--->of\n","tokenizing--->token\n","or--->or\n","splitting--->split\n","a--->a\n","string--->string\n",",--->,\n","text--->text\n","into--->into\n","a--->a\n","list--->list\n","of--->of\n","tokens--->token\n",".--->.\n","One--->one\n","can--->can\n","think--->think\n","of--->of\n","token--->token\n","as--->as\n","parts--->part\n","like--->like\n","a--->a\n","word--->word\n","is--->is\n","a--->a\n","token--->token\n","in--->in\n","a--->a\n","sentence--->sentenc\n",",--->,\n","and--->and\n","a--->a\n","sentence--->sentenc\n","is--->is\n","a--->a\n","token--->token\n","in--->in\n","a--->a\n","paragraph--->paragraph\n",".--->.\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]}]},{"cell_type":"markdown","source":["##Lemmatization\n","Lemmatization involves grouping together the inflected forms of the same word. This way, we can reach out to the base form of any word which will be meaningful in nature. The base from here is called the Lemma."],"metadata":{"id":"LJK2w6a7CJF1"}},{"cell_type":"markdown","source":["###WORDNET LEMMATIZATION"],"metadata":{"id":"OMrgz0yLohAo"}},{"cell_type":"code","source":[" from nltk.stem import WordNetLemmatizer\n"," wl=WordNetLemmatizer()\n"," for i in words:\n","  print(i+\"--->\"+wl.lemmatize(i,pos='v'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f-pBW4RIoj94","executionInfo":{"status":"ok","timestamp":1720589683683,"user_tz":-330,"elapsed":1678,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}},"outputId":"e706a289-8d35-4300-e60b-7e41408dc13f"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenization--->Tokenization\n","is--->be\n","the--->the\n","process--->process\n","of--->of\n","tokenizing--->tokenizing\n","or--->or\n","splitting--->split\n","a--->a\n","string--->string\n",",--->,\n","text--->text\n","into--->into\n","a--->a\n","list--->list\n","of--->of\n","tokens--->tokens\n",".--->.\n","One--->One\n","can--->can\n","think--->think\n","of--->of\n","token--->token\n","as--->as\n","parts--->part\n","like--->like\n","a--->a\n","word--->word\n","is--->be\n","a--->a\n","token--->token\n","in--->in\n","a--->a\n","sentence--->sentence\n",",--->,\n","and--->and\n","a--->a\n","sentence--->sentence\n","is--->be\n","a--->a\n","token--->token\n","in--->in\n","a--->a\n","paragraph--->paragraph\n",".--->.\n"]}]},{"cell_type":"code","source":["nltk.download('wordnet')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EtGGnLe2qUSV","executionInfo":{"status":"ok","timestamp":1720589684174,"user_tz":-330,"elapsed":7,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}},"outputId":"cc23f082-9f85-49f9-bfb8-905968fe9e49"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["print(wl.lemmatize(\"tokenizing\",pos='s'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_qumJnxGqkyD","executionInfo":{"status":"ok","timestamp":1720589684174,"user_tz":-330,"elapsed":6,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}},"outputId":"2cd3f43b-5178-4ebe-dd41-27fd95a46dd6"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["tokenizing\n"]}]},{"cell_type":"markdown","source":["#ONE HOT ENCODING"],"metadata":{"id":"fD7Kw7JKYeIw"}},{"cell_type":"markdown","source":["One-hot encoding is a process used in machine learning and data processing to convert categorical data into a binary matrix. This technique is particularly useful for algorithms that cannot work directly with categorical data and require numerical input. Here's how it works:\n","\n","Steps for One-Hot Encoding:\n","\n","1.Identify Categorical Variables: Determine which variables in your dataset are categorical.\n","\n","2.Create Binary Columns: For each unique category within a categorical variable, create a new binary column.\n","\n","3.Assign Binary Values: Assign a 1 or 0 to these columns to indicate the presence or absence of the category for each observation."],"metadata":{"id":"7sIjYC-GE2wV"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Sample categorical data\n","\n","data = {'fruit': ['apple', 'orange', 'banana', 'apple']}\n","\n","df = pd.DataFrame(data)\n","print(df)\n","\n","# One Hot Encoding using Pandas get_dummies\n","\n","encoded_df = pd.get_dummies(df, columns=['fruit'])\n","encoded_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":261},"id":"_GRIutEaX6O7","executionInfo":{"status":"ok","timestamp":1720589685787,"user_tz":-330,"elapsed":1616,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}},"outputId":"8c049834-4fa3-411e-c686-d8406763085f"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["    fruit\n","0   apple\n","1  orange\n","2  banana\n","3   apple\n"]},{"output_type":"execute_result","data":{"text/plain":["   fruit_apple  fruit_banana  fruit_orange\n","0         True         False         False\n","1        False         False          True\n","2        False          True         False\n","3         True         False         False"],"text/html":["\n","  <div id=\"df-ae9bd13a-21b9-4768-9dfa-68604b76454e\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>fruit_apple</th>\n","      <th>fruit_banana</th>\n","      <th>fruit_orange</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ae9bd13a-21b9-4768-9dfa-68604b76454e')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-ae9bd13a-21b9-4768-9dfa-68604b76454e button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-ae9bd13a-21b9-4768-9dfa-68604b76454e');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-2fb44caa-1843-4730-9af8-523ded9780ff\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2fb44caa-1843-4730-9af8-523ded9780ff')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-2fb44caa-1843-4730-9af8-523ded9780ff button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_40a2f762-944c-470b-a4b9-c14cfc358890\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('encoded_df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_40a2f762-944c-470b-a4b9-c14cfc358890 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('encoded_df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"encoded_df","summary":"{\n  \"name\": \"encoded_df\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"fruit_apple\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          false,\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fruit_banana\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true,\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fruit_orange\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true,\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["data = [['apple'], ['orange'], ['banana'], ['apple']]\n","\n","from sklearn.preprocessing import OneHotEncoder\n","encoder=OneHotEncoder(sparse_output=False)\n","encoder.fit(data)\n","encoded_data=encoder.transform(data)\n","print(encoded_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JlFIt5S9Zte8","executionInfo":{"status":"ok","timestamp":1720589685787,"user_tz":-330,"elapsed":32,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}},"outputId":"df703973-f92c-4ad1-ef63-8888baf096ff"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1. 0. 0.]\n"," [0. 0. 1.]\n"," [0. 1. 0.]\n"," [1. 0. 0.]]\n"]}]},{"cell_type":"markdown","source":["ONE HOT ENCODING ON SENTENCE"],"metadata":{"id":"QpOFYGmkFgnN"}},{"cell_type":"code","source":["from sklearn.preprocessing import OneHotEncoder\n","import numpy as np"],"metadata":{"id":"SPBnfnzlFeLV","executionInfo":{"status":"ok","timestamp":1720589685787,"user_tz":-330,"elapsed":29,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["sentence = \"One hot encoding is a process used to convert categorical data into a binary matrix.\""],"metadata":{"id":"nYOM9_ZvFnjB","executionInfo":{"status":"ok","timestamp":1720589685787,"user_tz":-330,"elapsed":28,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# Tokenize the sentence into words\n","words = sentence.split()\n","words"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RajEDDhWGDW4","executionInfo":{"status":"ok","timestamp":1720589685788,"user_tz":-330,"elapsed":29,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}},"outputId":"ac3abe62-db9f-46b9-dac2-2ce792bcb5a2"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['One',\n"," 'hot',\n"," 'encoding',\n"," 'is',\n"," 'a',\n"," 'process',\n"," 'used',\n"," 'to',\n"," 'convert',\n"," 'categorical',\n"," 'data',\n"," 'into',\n"," 'a',\n"," 'binary',\n"," 'matrix.']"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["# Create the vocabulary\n","vocab = list(set(words))"],"metadata":{"id":"SUJXpYU9GVAi","executionInfo":{"status":"ok","timestamp":1720589685788,"user_tz":-330,"elapsed":27,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["vocab"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BsPgfsg6GZcM","executionInfo":{"status":"ok","timestamp":1720589685788,"user_tz":-330,"elapsed":27,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}},"outputId":"1b7f9d96-8f36-4655-ada1-44a378a882b5"},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['used',\n"," 'matrix.',\n"," 'to',\n"," 'convert',\n"," 'binary',\n"," 'process',\n"," 'into',\n"," 'is',\n"," 'data',\n"," 'categorical',\n"," 'a',\n"," 'encoding',\n"," 'hot',\n"," 'One']"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["# Initialize the OneHotEncoder\n","onehot_encoder = OneHotEncoder(sparse=False)"],"metadata":{"id":"aBP5ZIyaGXIg","executionInfo":{"status":"ok","timestamp":1720589685788,"user_tz":-330,"elapsed":25,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["# Fit the encoder and transform the words\n","onehot_encoded = onehot_encoder.fit_transform(np.array(words).reshape(-1, 1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_89Y728uGXgZ","executionInfo":{"status":"ok","timestamp":1720589685788,"user_tz":-330,"elapsed":24,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}},"outputId":"a47c7ac9-8b40-474a-cf70-e2f81b5de2ba"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["# Display the result\n","print(\"Vocabulary:\", vocab)\n","print(\"One-Hot Encoded Vectors:\\n\", onehot_encoded)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f373ZvM8GvF8","executionInfo":{"status":"ok","timestamp":1720589685788,"user_tz":-330,"elapsed":23,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}},"outputId":"677f9997-0e14-4033-c89b-cd4066b99093"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary: ['used', 'matrix.', 'to', 'convert', 'binary', 'process', 'into', 'is', 'data', 'categorical', 'a', 'encoding', 'hot', 'One']\n","One-Hot Encoded Vectors:\n"," [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"oVJLrrZYGwBV","executionInfo":{"status":"ok","timestamp":1720589685788,"user_tz":-330,"elapsed":21,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["###TF_IDF VECTORIZATION\n","TF-IDF (Term Frequency-Inverse Document Frequency) is a statistical measure used to evaluate the importance of a word in a document relative to a collection of documents (or corpus). It is often used in information retrieval and text mining to reflect how important a word is to\n","\n","\n","\n","\n","\n"],"metadata":{"id":"OQvrjKO6nlCs"}},{"cell_type":"markdown","source":["**Term Frequency (TF)**: Measures how frequently a term appears in a document. It can be calculated as:\n","\n","**TF(𝑡,𝑑)=Number of times term 𝑡 appears in document 𝑑/total number of terms in document 𝑑**\n","\n","\n","**Inverse Document Frequency (IDF)**: Measures how important a term is in the entire corpus. It is calculated as:\n","\n","**IDF(𝑡)=log(Total number of documents/Number of documents containing term 𝑡)**\n","\n","The TF-IDF score for a term in a document is the product of its TF and IDF scores:\n","\n","**TF-IDF(𝑡,𝑑)=TF(𝑡,𝑑)×IDF(𝑡)**"],"metadata":{"id":"s19evj5wJtA8"}},{"cell_type":"code","source":[],"metadata":{"id":"p7_DApWuJsAh","executionInfo":{"status":"ok","timestamp":1720589685788,"user_tz":-330,"elapsed":20,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["d1=\"Tokenization is the process of tokenizing or splitting a string, text into a list of tokens.\"\n","d2=\"One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph.\""],"metadata":{"id":"f_vKBbf0nkMv","executionInfo":{"status":"ok","timestamp":1720589685788,"user_tz":-330,"elapsed":20,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize\n","from sklearn.feature_extraction.text import TfidfVectorizer"],"metadata":{"id":"5v74Fxz9vg_c","executionInfo":{"status":"ok","timestamp":1720589685788,"user_tz":-330,"elapsed":20,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["doc_corpus=[d1,d2]\n","print(doc_corpus)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q3Of308leDEN","executionInfo":{"status":"ok","timestamp":1720589685788,"user_tz":-330,"elapsed":20,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}},"outputId":"4fc9c592-dda5-4ea2-95aa-bdf3f4ff5bb1"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["['Tokenization is the process of tokenizing or splitting a string, text into a list of tokens.', 'One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph.']\n"]}]},{"cell_type":"code","source":["vec=TfidfVectorizer(stop_words='english')\n","matrix=vec.fit_transform(doc_corpus)\n","print(\"Feature Names n\",vec.get_feature_names_out())\n"],"metadata":{"id":"jmpVd119vlh0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720589685789,"user_tz":-330,"elapsed":19,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}},"outputId":"57a20646-c90e-42d7-8421-4e13e569d7cf"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Feature Names n ['like' 'list' 'paragraph' 'parts' 'process' 'sentence' 'splitting'\n"," 'string' 'text' 'think' 'token' 'tokenization' 'tokenizing' 'tokens'\n"," 'word']\n"]}]},{"cell_type":"code","source":["print(\"Sparse Matrix n\",matrix.shape,\"n\",matrix.toarray())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KzUGf0fIeabk","executionInfo":{"status":"ok","timestamp":1720589685789,"user_tz":-330,"elapsed":17,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}},"outputId":"8d1115b6-c156-4211-a658-586c1bb3632c"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Sparse Matrix n (2, 15) n [[0.         0.35355339 0.         0.         0.35355339 0.\n","  0.35355339 0.35355339 0.35355339 0.         0.         0.35355339\n","  0.35355339 0.35355339 0.        ]\n"," [0.23570226 0.         0.23570226 0.23570226 0.         0.47140452\n","  0.         0.         0.         0.23570226 0.70710678 0.\n","  0.         0.         0.23570226]]\n"]}]},{"cell_type":"markdown","source":["#BOW"],"metadata":{"id":"rh2-xQvkfC0O"}},{"cell_type":"code","source":["import nltk\n","\n","paragraph =  \"\"\"I have three visions for India. In 3000 years of our history, people from all over\n","               the world have come and invaded us, captured our lands, conquered our minds.\n","               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n","               the French, the Dutch, all of them came and looted us, took over what was ours.\n","               Yet we have not done this to any other nation. We have not conquered anyone.\n","               We have not grabbed their land, their culture,\n","               their history and tried to enforce our way of life on them.\n","               Why? Because we respect the freedom of others.That is why my\n","               first vision is that of freedom. I believe that India got its first vision of\n","               this in 1857, when we started the War of Independence. It is this freedom that\n","               we must protect and nurture and build on. If we are not free, no one will respect us.\n","               My second vision for India’s development. For fifty years we have been a developing nation.\n","               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n","               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n","               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n","               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n","               I have a third vision. India must stand up to the world. Because I believe that unless India\n","               stands up to the world, no one will respect us. Only strength respects strength. We must be\n","               strong not only as a military power but also as an economic power. Both must go hand-in-hand.\n","               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of\n","               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n","               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.\n","               I see four milestones in my career\"\"\"\n","\n","\n","\n"],"metadata":{"id":"QOIHLzQmv4vR","executionInfo":{"status":"ok","timestamp":1720589685789,"user_tz":-330,"elapsed":15,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["# Cleaning the texts\n","import re\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","from nltk.stem import WordNetLemmatizer"],"metadata":{"id":"Vz5s9Ng1wLJf","executionInfo":{"status":"ok","timestamp":1720589685789,"user_tz":-330,"elapsed":15,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["ps = PorterStemmer()\n","wordnet=WordNetLemmatizer()\n","sentences = nltk.sent_tokenize(paragraph)\n","corpus = []"],"metadata":{"id":"cbf0nlAqAvlM","executionInfo":{"status":"ok","timestamp":1720589685789,"user_tz":-330,"elapsed":15,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["for i in range(len(sentences)):\n","    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n","    review = review.lower()\n","    review = review.split()\n","    review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n","    review = ' '.join(review)\n","    corpus.append(review)"],"metadata":{"id":"3le1kzTH_MDt","executionInfo":{"status":"ok","timestamp":1720589685789,"user_tz":-330,"elapsed":14,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["# Creating the Bag of Words model\n","from sklearn.feature_extraction.text import CountVectorizer\n","cv = CountVectorizer(max_features = 1500)\n","X = cv.fit_transform(corpus).toarray()\n","print(X)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mj2vjd8IBAzi","executionInfo":{"status":"ok","timestamp":1720589685789,"user_tz":-330,"elapsed":14,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}},"outputId":"11756c87-f072-4536-9615-e8585a629646"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0 0 0 ... 0 0 0]\n"," [0 0 0 ... 1 1 0]\n"," [0 1 0 ... 0 0 0]\n"," ...\n"," [0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]]\n"]}]},{"cell_type":"markdown","source":["#WORD2VEC\n","Word2Vec creates vectors of the words that are distributed numerical representations of word features – these word features could comprise of words that represent the context of the individual words present in our vocabulary. Word embeddings eventually help in establishing the association of a word with another similar meaning word through the created vectors"],"metadata":{"id":"54dimA2xCFAc"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","from gensim.models import Word2Vec\n","from nltk.corpus import stopwords\n","\n","import re"],"metadata":{"id":"aNfiNoFlCIUl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720589686999,"user_tz":-330,"elapsed":1222,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}},"outputId":"0fab005b-0e65-4fee-f487-e118adb114e8"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"code","source":["paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over\n","               the world have come and invaded us, captured our lands, conquered our minds.\n","               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n","               the French, the Dutch, all of them came and looted us, took over what was ours.\n","               Yet we have not done this to any other nation. We have not conquered anyone.\n","               We have not grabbed their land, their culture,\n","               their history and tried to enforce our way of life on them.\"\"\"\n"],"metadata":{"id":"Oqp9p3fcCLu2","executionInfo":{"status":"ok","timestamp":1720589687000,"user_tz":-330,"elapsed":23,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["# Preprocessing the data\n","text = re.sub(r'\\[[0-9]*\\]',' ',paragraph)\n","text = re.sub(r'\\s+',' ',text)\n","text = text.lower()\n","text = re.sub(r'\\d',' ',text)\n","text = re.sub(r'\\s+',' ',text)"],"metadata":{"id":"22gBpsIACTAh","executionInfo":{"status":"ok","timestamp":1720589687000,"user_tz":-330,"elapsed":23,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["# Preparing the dataset\n","sentences = nltk.sent_tokenize(text)\n","\n","sentences = [nltk.word_tokenize(sentence) for sentence in sentences]"],"metadata":{"id":"6oS14d8bCU5X","executionInfo":{"status":"ok","timestamp":1720589687000,"user_tz":-330,"elapsed":23,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["for i in range(len(sentences)):\n","    sentences[i] = [word for word in sentences[i] if word not in stopwords.words('english')]"],"metadata":{"id":"oD8hs6YJCXRR","executionInfo":{"status":"ok","timestamp":1720589687000,"user_tz":-330,"elapsed":22,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["\n","# Training the Word2Vec model\n","model = Word2Vec(sentences, min_count=1)"],"metadata":{"id":"jONqReHbCZjp","executionInfo":{"status":"ok","timestamp":1720589687000,"user_tz":-330,"elapsed":22,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["words = model.wv.key_to_index\n","for i in words:\n","  print(words[i],\"------->\",i)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"etOKoyDuCbaF","executionInfo":{"status":"ok","timestamp":1720589687000,"user_tz":-330,"elapsed":22,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}},"outputId":"b4f0e1f1-3bc5-44ac-d4ac-4a53d65416b2"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["0 -------> ,\n","1 -------> .\n","2 -------> conquered\n","3 -------> history\n","4 -------> us\n","5 -------> life\n","6 -------> invaded\n","7 -------> alexander\n","8 -------> minds\n","9 -------> lands\n","10 -------> captured\n","11 -------> world\n","12 -------> come\n","13 -------> greeks\n","14 -------> people\n","15 -------> years\n","16 -------> india\n","17 -------> visions\n","18 -------> onwards\n","19 -------> turks\n","20 -------> way\n","21 -------> done\n","22 -------> enforce\n","23 -------> tried\n","24 -------> culture\n","25 -------> land\n","26 -------> grabbed\n","27 -------> anyone\n","28 -------> nation\n","29 -------> yet\n","30 -------> moguls\n","31 -------> took\n","32 -------> looted\n","33 -------> came\n","34 -------> dutch\n","35 -------> french\n","36 -------> british\n","37 -------> portuguese\n","38 -------> three\n"]}]},{"cell_type":"code","source":["# Finding Word Vectors\n","vector = model.wv['grabbed']\n","print(vector)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eAVwMEaVEKZF","executionInfo":{"status":"ok","timestamp":1720589687000,"user_tz":-330,"elapsed":19,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}},"outputId":"7ebab39c-a2d0-4401-aab2-271c5f08d666"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["[-6.9625224e-03 -2.4570711e-03 -8.0231009e-03  7.4998569e-03\n","  6.1294287e-03  5.2598049e-03  8.3753048e-03 -6.9806911e-04\n"," -9.3106739e-03  9.1157416e-03 -4.9290746e-03  7.8482348e-03\n","  5.5324221e-03 -1.0805655e-03 -7.6641268e-03 -1.4598919e-03\n","  6.2540262e-03 -6.9685564e-03  1.4447495e-03 -7.9497863e-03\n","  8.7225642e-03 -2.8563982e-03  9.4370665e-03 -5.7080411e-03\n"," -9.7175669e-03 -8.6270161e-03 -4.0752478e-03  4.7115544e-03\n"," -2.4033277e-04  9.2256609e-03  3.1082083e-03  3.7466774e-03\n","  2.9941848e-03  8.1482371e-03 -2.3958571e-03  7.4063162e-03\n"," -9.5373588e-03  2.9241510e-03 -6.8165950e-04  4.5327315e-04\n","  6.8427618e-03 -2.8411502e-03 -2.3576040e-03 -9.9652709e-05\n"," -5.0032663e-04 -3.5738496e-03  6.2434361e-03 -6.5583410e-03\n","  7.8907264e-03 -9.2743649e-05  2.6102422e-03  3.2235659e-03\n"," -2.8280876e-04  1.7051279e-03 -3.1412125e-03  4.7563668e-03\n","  2.4294248e-04 -3.2808175e-03 -8.7157963e-03 -9.9969180e-03\n","  3.0956228e-04 -5.7464228e-03 -1.1101884e-03 -4.2061582e-03\n"," -8.6368443e-03  1.0607544e-03  5.9139063e-03 -2.2116711e-03\n"," -7.1681915e-03  3.1511895e-03 -3.8399582e-04 -5.5224523e-03\n"," -1.1074157e-03 -6.3931098e-04 -3.1844485e-03 -9.9567678e-03\n","  7.6395771e-03  3.7241154e-03 -2.5275529e-03  7.3069758e-03\n","  4.5250013e-04  7.1737207e-03 -1.5458259e-03  7.4918591e-03\n"," -4.3698459e-05 -6.0775732e-03 -4.7164080e-03  9.6283685e-03\n","  5.8183004e-04  1.0264330e-03  8.4511237e-03 -6.2869694e-03\n"," -1.7635203e-03 -8.1788227e-03 -6.6775838e-03 -8.5809706e-03\n","  3.9298506e-03  2.7409478e-03  5.6146164e-03  2.5735244e-03]\n"]}]},{"cell_type":"code","source":["# Most similar words\n","similar = model.wv.most_similar('nation')\n","for i in similar:\n","  print(i)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"59jxEmbHCdut","executionInfo":{"status":"ok","timestamp":1720589687001,"user_tz":-330,"elapsed":18,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}},"outputId":"ff7be098-39b1-4f05-bd22-7a3573badd18"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["('invaded', 0.30389490723609924)\n","(',', 0.19518142938613892)\n","('turks', 0.1888754814863205)\n","('grabbed', 0.16689462959766388)\n","('india', 0.14204353094100952)\n","('lands', 0.1263914704322815)\n","('moguls', 0.11590827256441116)\n","('three', 0.09515701234340668)\n","('british', 0.08005909621715546)\n","('dutch', 0.07988414168357849)\n"]}]},{"cell_type":"code","source":["from sklearn.linear_model import LinearRegression\n","reg=LinearRegression()"],"metadata":{"id":"V1aJmUgRC9p7","executionInfo":{"status":"ok","timestamp":1720592626789,"user_tz":-330,"elapsed":450,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","X_train,X_test,y_train,y_test=train_test_split(vector,test_size=0.2,random_state=2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":158},"id":"tpBlb8YoT-_c","executionInfo":{"status":"error","timestamp":1720592883519,"user_tz":-330,"elapsed":461,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}},"outputId":"fc9781a6-d8e0-4307-b864-5545af245ef4"},"execution_count":54,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"not enough values to unpack (expected 4, got 2)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-54-a221cd15e42e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"]}]},{"cell_type":"markdown","source":["1.create document for all embeddings\n","2.simple linear regression model on all with complete detail\n","3.2 sql questions\n","4.python question\n","5.remind to connect at 6\n"],"metadata":{"id":"FnBSl06R_A7Q"}},{"cell_type":"code","source":[],"metadata":{"id":"vnD5gc5izTM5","executionInfo":{"status":"aborted","timestamp":1720589687005,"user_tz":-330,"elapsed":14,"user":{"displayName":"Ankit Jangid","userId":"14116634465653545240"}}},"execution_count":null,"outputs":[]}]}